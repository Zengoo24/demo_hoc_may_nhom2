import streamlit as st
import cv2
import mediapipe as mp
import numpy as np
import json
import av
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, WebRtcMode, RTCConfiguration
import joblib
from collections import deque
from PIL import Image
import time
from ultralytics import YOLO 

# Th√™m khai b√°o mp_drawing v√† mp_hands
mp_drawing = mp.solutions.drawing_utils
mp_hands = mp.solutions.hands

# ======================================================================
# I. C·∫§U H√åNH V√Ä H·∫∞NG S·ªê CHUNG
# ======================================================================

# --- C·∫•u h√¨nh chung ---
EPS = 1e-8
NEW_WIDTH, NEW_HEIGHT = 640, 480

# --- C·∫•u h√¨nh Drowsiness (Face Mesh) ---
MODEL_PATH = "softmax_model_best1.pkl"
SCALER_PATH = "scale1.pkl"
LABEL_MAP_PATH = "label_map_6cls.json"
SMOOTH_WINDOW = 5
BLINK_THRESHOLD = 0.20
N_FEATURES = 10 

# --- C·∫•u h√¨nh Wheel (Hands) ---
WHEEL_MODEL_PATH = "softmax_wheel_model.pkl"
WHEEL_SCALER_PATH = "scaler_wheel.pkl"
YOLO_MODEL_PATH = "best (1).pt" 
EXPECTED_WHEEL_FEATURES = 128 # K√≠ch th∆∞·ªõc ƒë·∫∑c tr∆∞ng m·ªõi

# ======================================================================
# II. C√ÅC H√ÄM T√çNH TO√ÅN C∆† B·∫¢N V√Ä T·∫¢I T√ÄI NGUY√äN
# ======================================================================

def softmax_predict(X, W, b):
    """Th·ª±c hi·ªán d·ª± ƒëo√°n Softmax (Face Mesh)."""
    logits = X @ W + b
    z = logits - np.max(logits, axis=1, keepdims=True)
    exp_z = np.exp(z)
    probs = exp_z / np.sum(exp_z, axis=1, keepdims=True)
    return np.argmax(probs, axis=1)

def softmax_wheel(z):
    """Th·ª±c hi·ªán Softmax chu·∫©n cho Hands/Wheel."""
    z -= np.max(z, axis=1, keepdims=True)
    exp_z = np.exp(z)
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)

@st.cache_resource
def get_mp_hands_instance():
    """T·∫°o instance MediaPipe Hands (cho x·ª≠ l√Ω ·∫£nh tƒ©nh V√¥ lƒÉng)."""
    return mp.solutions.hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)

@st.cache_resource
def load_yolo_model(model_path):
    """T·∫£i m√¥ h√¨nh YOLOv8 ƒë√£ train."""
    try:
        return YOLO(model_path)
    except Exception as e:
        st.error(f"L·ªñI T·∫¢I YOLO: Kh√¥ng t√¨m th·∫•y file {model_path} ho·∫∑c l·ªói kh·ªüi t·∫°o: {e}")
        return None

@st.cache_resource
def load_assets():
    """T·∫£i t·∫•t c·∫£ tham s·ªë m√¥ h√¨nh, scaler v√† label map."""
    try:
        # --- 1. T·∫£i M√¥ h√¨nh Face Mesh ---
        with open(MODEL_PATH, "rb") as f:
            model_data = joblib.load(f)
            W = model_data["W"]
            b = model_data["b"]
        with open(SCALER_PATH, "rb") as f:
            scaler_data = joblib.load(f)
            mean_data = scaler_data["X_mean"]
            std_data = scaler_data["X_std"]
        with open(LABEL_MAP_PATH, "r") as f:
            label_map = json.load(f)
            id2label = {int(v): k for k, v in label_map.items()}

        if W.shape[0] != N_FEATURES:
            st.error(f"L·ªñI KH√îNG T∆Ø∆†NG TH√çCH: M√¥ h√¨nh FACE MESH y√™u c·∫ßu {W.shape[0]} ƒë·∫∑c tr∆∞ng, nh∆∞ng ·ª©ng d·ª•ng n√†y tr√≠ch xu·∫•t {N_FEATURES} ƒë·∫∑c tr∆∞ng. Vui l√≤ng ki·ªÉm tra l·∫°i file model!")
            st.stop()

        # --- 2. T·∫£i M√¥ h√¨nh Wheel/Hands ---
        with open(WHEEL_MODEL_PATH, "rb") as f:
            wheel_model_data = joblib.load(f)
            W_WHEEL = wheel_model_data["W"]
            b_WHEEL = wheel_model_data["b"]
            CLASS_NAMES_WHEEL = wheel_model_data.get("classes", ["off-wheel", "on-wheel"])

        with open(WHEEL_SCALER_PATH, "rb") as f:
            wheel_scaler_data = joblib.load(f)
            X_mean_WHEEL = wheel_scaler_data["X_mean"]
            X_std_WHEEL = wheel_scaler_data["X_std"]
        
        # Ki·ªÉm tra k√≠ch th∆∞·ªõc ƒë·∫∑c tr∆∞ng c·ªßa m√¥ h√¨nh Wheel
        if W_WHEEL.shape[0] != EXPECTED_WHEEL_FEATURES:
            st.error(f"L·ªñI KH√îNG T∆Ø∆†NG TH√çCH: M√¥ h√¨nh V√î LƒÇNG y√™u c·∫ßu {W_WHEEL.shape[0]} ƒë·∫∑c tr∆∞ng, nh∆∞ng code d·ª± ki·∫øn {EXPECTED_WHEEL_FEATURES}. Vui l√≤ng ki·ªÉm tra v√† hu·∫•n luy·ªán l·∫°i m√¥ h√¨nh Softmax V√¥ LƒÉng.")
            st.stop()

        # --- 3. T·∫£i m√¥ h√¨nh YOLOv8 ---
        yolo_model = load_yolo_model(YOLO_MODEL_PATH)
        if yolo_model is None: 
            st.stop()
            
        # --- 4. Kh·ªüi t·∫°o Face Mesh (Global Reference) ---
        mp_face_mesh = mp.solutions.face_mesh
        
        return W, b, mean_data, std_data, id2label, W_WHEEL, b_WHEEL, X_mean_WHEEL, X_std_WHEEL, CLASS_NAMES_WHEEL, yolo_model

    except FileNotFoundError as e:
        st.error(f"L·ªñI FILE: Kh√¥ng t√¨m th·∫•y file t√†i nguy√™n. Vui l√≤ng ki·ªÉm tra ƒë∆∞·ªùng d·∫´n: {e.filename}")
        st.stop()
    except Exception as e:
        st.error(f"L·ªñ·ªñI LOAD D·ªÆ LI·ªÜU: Chi ti·∫øt: {e}")
        st.stop()

# T·∫£i t√†i s·∫£n (Ch·∫°y m·ªôt l·∫ßn)
W, b, mean, std, id2label, W_WHEEL, b_WHEEL, X_mean_WHEEL, X_std_WHEEL, CLASS_NAMES_WHEEL, YOLO_MODEL = load_assets()
mp_face_mesh = mp.solutions.face_mesh # Global reference

# ======================================================================
# III. H√ÄM TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG KHU√îN M·∫∂T (FACE MESH)
# ======================================================================

EYE_LEFT_IDX = np.array([33, 159, 145, 133, 153, 144])
EYE_RIGHT_IDX = np.array([362, 386, 374, 263, 380, 385])
MOUTH_IDX = np.array([61, 291, 0, 17, 78, 308])

def eye_aspect_ratio(landmarks, left=True):
    idx = EYE_LEFT_IDX if left else EYE_RIGHT_IDX
    pts = landmarks[idx, :2]
    A = np.linalg.norm(pts[1] - pts[5])
    B = np.linalg.norm(pts[2] - pts[4])
    C = np.linalg.norm(pts[0] - pts[3])
    return (A + B) / (2.0 * (C + EPS))

def mouth_aspect_ratio(landmarks):
    pts = landmarks[MOUTH_IDX, :2]
    A = np.linalg.norm(pts[0] - pts[1])
    B = np.linalg.norm(pts[4] - pts[5])
    C = np.linalg.norm(pts[2] - pts[3])
    return (A + B) / (2.0 * (C + EPS))

def head_pose_yaw_pitch_roll(landmarks):
    left_eye = landmarks[33][:2]
    right_eye = landmarks[263][:2]
    nose = landmarks[1][:2]
    chin = landmarks[152][:2]

    dx = right_eye[0] - left_eye[0]
    dy = right_eye[1] - left_eye[1]
    roll = np.degrees(np.arctan2(dy, dx + EPS))

    interocular = np.linalg.norm(right_eye - left_eye) + EPS
    eyes_center = (left_eye + right_eye) / 2.0
    yaw = np.degrees(np.arctan2((nose[0] - eyes_center[0]), interocular))

    baseline = chin - eyes_center
    pitch = np.degrees(np.arctan2((nose[1] - eyes_center[1]), (np.linalg.norm(baseline) + EPS)))
    return yaw, pitch, roll

def get_extra_features(landmarks):
    nose, chin = landmarks[1], landmarks[152]
    angle_pitch_extra = np.degrees(np.arctan2(chin[1] - nose[1], (chin[2] - nose[2]) + EPS))
    forehead_y = np.mean(landmarks[[10, 338, 297, 332, 284], 1])
    return angle_pitch_extra, forehead_y

# ======================================================================
# IV. H√ÄM TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG V√î LƒÇNG (WHEEL/HANDS - 128 FEATURES)
# ======================================================================

def detect_wheel_yolo(frame, yolo_model):
    """Ph√°t hi·ªán v√¥ lƒÉng b·∫±ng YOLOv8 v√† tr·∫£ v·ªÅ (bbox, x, y, r)."""
    # classes=[0] gi·∫£ ƒë·ªãnh 'steering_wheel' l√† l·ªõp 0
    results = yolo_model(frame, verbose=False, conf=0.5, classes=[0]) 
    
    for result in results:
        boxes = result.boxes
        if len(boxes) > 0:
            box = boxes[0].xyxy[0].cpu().numpy().astype(int)
            x_min, y_min, x_max, y_max = box
            
            x_w = (x_min + x_max) // 2
            y_w = (y_min + y_max) // 2
            r_w = int((x_max - x_min + y_max - y_min) / 4) 
            
            return (x_min, y_min, x_max, y_max), (x_w, y_w, r_w)
            
    return None, None

def extract_wheel_features(image, hands_processor, wheel_coords):
    """
    Tr√≠ch xu·∫•t 128 ƒë·∫∑c tr∆∞ng tay cho m√¥ h√¨nh Softmax (Bao g·ªìm t·ªça ƒë·ªô tuy·ªát ƒë·ªëi,
    kho·∫£ng c√°ch t·ªõi t√¢m v√† c√°c ƒë·∫∑c tr∆∞ng t∆∞∆°ng ƒë·ªëi c·ªßa ƒë·∫ßu ng√≥n tay).
    """
    xw, yw, rw = wheel_coords
    h, w, _ = image.shape
    feats_all = []

    with mp_hands.Hands(static_image_mode=True, max_num_hands=2) as hands:
        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        res = hands.process(rgb)
        
        # N·∫øu kh√¥ng t√¨m th·∫•y tay, tr·∫£ v·ªÅ None (lu·∫≠t "Kh√¥ng tay = R·ªúI" s·∫Ω x·ª≠ l√Ω)
        if not res.multi_hand_landmarks: 
            return None 

        for hand_landmarks in res.multi_hand_landmarks:
            feats = []
            normalized_coords = []
            
            # 1. Tr√≠ch xu·∫•t T·ªça ƒë·ªô chu·∫©n h√≥a (63 ƒë·∫∑c tr∆∞ng: 21 * x,y,z)
            for lm in hand_landmarks.landmark:
                feats.extend([lm.x, lm.y, lm.z])
                normalized_coords.append(np.array([lm.x, lm.y])) 

            # 2. ƒê·∫∑c tr∆∞ng Kho·∫£ng c√°ch ƒë·∫øn t√¢m v√¥ lƒÉng (1 ƒë·∫∑c tr∆∞ng)
            hx = hand_landmarks.landmark[0].x * w
            hy = hand_landmarks.landmark[0].y * h
            dist_to_center = np.sqrt((xw - hx) ** 2 + (yw - hy) ** 2)
            feats.append(dist_to_center / (rw + EPS))

            # --- TH√äM C√ÅC ƒê·∫∂C TR∆ØNG N√ÇNG CAO (64 + 10 + 10 = 84 features per hand) ---
            
            # a) ƒê·∫∑c tr∆∞ng v·ªã tr√≠ t∆∞∆°ng ƒë·ªëi c·ªßa c√°c ƒë·∫ßu ng√≥n tay so v·ªõi t√¢m v√¥ lƒÉng (10 ƒë·∫∑c tr∆∞ng)
            tip_indices = [4, 8, 12, 16, 20] 
            
            for i in tip_indices:
                lm_tip = hand_landmarks.landmark[i]
                
                tip_x = lm_tip.x * w
                tip_y = lm_tip.y * h
                
                # Kho·∫£ng c√°ch t∆∞∆°ng ƒë·ªëi
                rel_dist = np.sqrt((xw - tip_x) ** 2 + (yw - tip_y) ** 2)
                feats.append(rel_dist / (rw + EPS))
                
                # G√≥c t∆∞∆°ng ƒë·ªëi
                angle = np.arctan2(tip_y - yw, tip_x - xw) / np.pi 
                feats.append(angle)

            # b) ƒê·∫∑c tr∆∞ng Kho·∫£ng c√°ch gi·ªØa c√°c ng√≥n tay (10 ƒë·∫∑c tr∆∞ng)
            # (5, 8) = Ng√≥n tr·ªè, (9, 12) = Ng√≥n gi·ªØa, v.v.
            pairs = [(5, 8), (9, 12), (13, 16), (17, 20), (0, 5)] 
            for i, j in pairs:
                p_i = normalized_coords[i]
                p_j = normalized_coords[j]
                
                distance = np.linalg.norm(p_i - p_j)
                feats.append(distance)
                
            feats_all.extend(feats)

        # ƒê·∫£m b·∫£o ƒë·ªß ƒë·ªô d√†i (128) cho m√¥ h√¨nh 2 tay
        expected_len = W_WHEEL.shape[0] 
        
        if len(feats_all) < expected_len:
            feats_all.extend([0.0] * (expected_len - len(feats_all)))
        
        feats_all = feats_all[:expected_len]

    return np.array(feats_all, dtype=np.float32)

# ======================================================================
# V. H√ÄM X·ª¨ L√ù ·∫¢NH Tƒ®NH (WHEEL)
# ======================================================================

def process_static_wheel_image(image_file, W_WHEEL, b_WHEEL, X_mean_WHEEL, X_std_WHEEL, CLASS_NAMES_WHEEL, YOLO_MODEL):
    img_pil = Image.open(image_file).convert('RGB')
    img_np = np.array(img_pil)
    img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
    
    # 1. PH√ÅT HI·ªÜN V√î LƒÇNG B·∫∞NG YOLO
    bbox_result, wheel_coords = detect_wheel_yolo(img_bgr, YOLO_MODEL)

    if wheel_coords is None:
        cv2.putText(img_bgr, "WHEEL NOT FOUND", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)
        return cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB), "WHEEL NOT FOUND"

    # 2. TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG (S·ª≠ d·ª•ng h√†m 128 features m·ªõi)
    # Kh√¥ng c·∫ßn truy·ªÅn hands_processor v√¨ n√≥ ƒë∆∞·ª£c t·∫°o l·∫°i b√™n trong extract_wheel_features
    features = extract_wheel_features(img_bgr, None, wheel_coords)
    
    final_predicted_class = "off-wheel" 

    # üõë LU·∫¨T C·ª®NG: KH√îNG TAY = R·ªúI üõë
    if features is None:
        final_predicted_class = "off-wheel"
        display_label = "ROI"
        final_color = (0, 0, 255) # ƒê·ªè
        text_to_display = "ROI"
    
    else:
        # 3. D·ª∞ ƒêO√ÅN SOFTMAX THU·∫¶N T√öY
        X_sample = features.reshape(1, -1)
        X_scaled = (X_sample - X_mean_WHEEL) / (X_std_WHEEL + EPS)
        z = X_scaled @ W_WHEEL + b_WHEEL
        probabilities = softmax_wheel(z)[0]
        
        predicted_index = np.argmax(probabilities)
        final_predicted_class = CLASS_NAMES_WHEEL[predicted_index]
        confidence = probabilities[predicted_index] * 100
        
        # 4. G√°n nh√£n hi·ªÉn th·ªã
        display_label = "CAM" if final_predicted_class == "on-wheel" else "KHONG CAM"
        final_color = (0, 255, 0) if final_predicted_class == "on-wheel" else (0, 0, 255)
        text_to_display = f"{display_label} ({confidence:.1f}%)"
        
        # V·∫Ω tay (landmarks) l√™n ·∫£nh BGR
        with mp_hands.Hands(static_image_mode=True, max_num_hands=2) as hands_drawer: 
            rgb_for_drawing = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
            res_for_drawing = hands_drawer.process(rgb_for_drawing)
            if res_for_drawing.multi_hand_landmarks:
                for hand_landmarks in res_for_drawing.multi_hand_landmarks:
                    mp_drawing.draw_landmarks(img_bgr, hand_landmarks, mp_hands.HAND_CONNECTIONS,
                                              mp_drawing.DrawingSpec(color=(255, 255, 0), thickness=2, circle_radius=2),
                                              mp_drawing.DrawingSpec(color=(255, 200, 0), thickness=2, circle_radius=2))

    # 5. V·∫Ω V√¥ lƒÉng
    x_min, y_min, x_max, y_max = bbox_result
    xw, yw, rw = wheel_coords
    
    cv2.rectangle(img_bgr, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2) # Bounding Box YOLO
    cv2.circle(img_bgr, (xw, yw), rw, (255, 0, 255), 2) # V√≤ng tr√≤n ∆∞·ªõc t√≠nh t·ª´ YOLO
    
    # 6. ƒê·∫∑t text hi·ªÉn th·ªã cu·ªëi c√πng
    cv2.putText(img_bgr, text_to_display, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.2, final_color, 3, cv2.LINE_AA)
    
    return cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB), final_predicted_class.upper()


# ======================================================================
# VII. L·ªöP X·ª¨ L√ù VIDEO LIVE (WEBRTC PROCESSOR)
# ======================================================================
class DrowsinessProcessor(VideoProcessorBase):
    def __init__(self):
        self.W = W; self.b = b; self.mean = mean; self.std = std; self.id2label = id2label
        self.face_mesh = mp_face_mesh.FaceMesh(
            max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5, min_tracking_confidence=0.5)
        self.pred_queue = deque(maxlen=SMOOTH_WINDOW)
        self.last_pred_label = "CHO DU LIEU VAO"
        self.N_FEATURES = N_FEATURES
        
        self.last_ear_avg = 0.4 
        self.last_pitch = 0.0
        self.pTime = time.time() 
        self.fps = 0

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        frame_array = frame.to_ndarray(format="bgr24")
        frame_resized = cv2.resize(frame_array, (NEW_WIDTH, NEW_HEIGHT))
        h, w = frame_resized.shape[:2]

        rgb_unflipped = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)

        results = self.face_mesh.process(rgb_unflipped)
        
        ear_avg = 0.0
        delta_ear_value_display = 0.0
        delta_pitch_value_display = 0.0
        
        predicted_label_frame = "NO FACE" 

        if results.multi_face_landmarks:
            landmarks = np.array([[p.x * w, p.y * h, p.z * w] for p in results.multi_face_landmarks[0].landmark])

            ear_l = eye_aspect_ratio(landmarks, True)
            ear_r = eye_aspect_ratio(landmarks, False)
            ear_avg = (ear_l + ear_r) / 2.0

            mar = mouth_aspect_ratio(landmarks)
            yaw, pitch, roll = head_pose_yaw_pitch_roll(landmarks)
            angle_pitch_extra, forehead_y = get_extra_features(landmarks)

            delta_ear_value_display = ear_avg - self.last_ear_avg
            delta_pitch_value_display = pitch - self.last_pitch

            self.last_ear_avg = ear_avg
            self.last_pitch = pitch
            
            if ear_avg < BLINK_THRESHOLD:
                predicted_label_frame = "blink"
            else:
                feats = np.array([ear_l, ear_r, mar, yaw, pitch, roll,
                                  angle_pitch_extra, delta_ear_value_display, forehead_y, delta_pitch_value_display], dtype=np.float32)

                feats_scaled = (feats - self.mean[:self.N_FEATURES]) / (self.std[:self.N_FEATURES] + EPS)
                pred_idx = softmax_predict(np.expand_dims(feats_scaled, axis=0), self.W, self.b)[0]
                predicted_label_frame = self.id2label.get(pred_idx, "UNKNOWN")

            self.pred_queue.append(predicted_label_frame)

        else:
            self.last_ear_avg = 0.4
            self.last_pitch = 0.0
            self.pred_queue.clear() 

        if len(self.pred_queue) > 0:
            self.last_pred_label = max(set(self.pred_queue), key=self.pred_queue.count)
        else:
            self.last_pred_label = "NO FACE"

        cTime = time.time()
        self.fps = 0.9 * self.fps + 0.1 * (1 / (cTime - self.pTime + EPS))
        self.pTime = cTime

        frame_display_bgr = frame_resized

        cv2.putText(frame_display_bgr, f"FPS: {int(self.fps)}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        cv2.putText(frame_display_bgr, f"State: {self.last_pred_label.upper()}", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 0), 3)

        return av.VideoFrame.from_ndarray(frame_display_bgr, format="bgr24")

# ======================================================================
# VIII. GIAO DI·ªÜN STREAMLIT CH√çNH
# ======================================================================
st.set_page_config(page_title="Demo Softmax", layout="wide")

tab1, tab2 = st.tabs(["üî¥ D·ª± ƒëo√°n Live Camera", "üöó Ki·ªÉm tra V√¥ LƒÉng (Tay)"])

with tab1:
    st.header("1. Nh·∫≠n di·ªán Tr·∫°ng th√°i Khu√¥n m·∫∑t (Live Camera)")
    st.warning("Vui l√≤ng ch·∫•p nh·∫≠n y√™u c·∫ßu truy c·∫≠p camera t·ª´ tr√¨nh duy·ªát c·ªßa b·∫°n.")
    st.markdown("---")

    col1, col2, col3 = st.columns([1, 4, 1])
    with col2:
        ICE_SERVERS = [
            {"urls": ["stun:stun.l.google.com:19302"]},
            {"urls": ["stun:stun1.l.google.com:19302"]},
            {"urls": ["stun:stun2.l.google.com:19302"]},
            {"urls": ["stun:stun.services.mozilla.com:3478"]},
            {"urls": ["turn:numb.viagenie.ca:3478"], "username": "webrtc@live.com", "credential": "muazkh"} 
        ]
        
        rtc_config = RTCConfiguration({"iceServers": ICE_SERVERS})

        webrtc_streamer(
            key="softmax_driver_live",
            mode=WebRtcMode.SENDRECV,
            rtc_configuration=rtc_config,
            video_processor_factory=DrowsinessProcessor,
            media_stream_constraints={"video": True, "audio": False},
            async_processing=True,
        )

with tab2:
    st.header("2. Ki·ªÉm tra V·ªã tr√≠ Tay (V√¥ LƒÉng)")
    st.warning(f"Nh·∫≠n di·ªán: {CLASS_NAMES_WHEEL}")
    st.markdown("### T·∫£i l√™n ·∫£nh tay c·∫ßm/r·ªùi v√¥ lƒÉng ƒë·ªÉ d·ª± ƒëo√°n")
    uploaded_wheel_file = st.file_uploader("Ch·ªçn m·ªôt ·∫£nh v√¥ lƒÉng (.jpg, .png)", type=["jpg", "png", "jpeg"], key="wheel_upload")

    if uploaded_wheel_file is not None:
        st.info("ƒêang x·ª≠ l√Ω ·∫£nh...")
        # üõë ƒê√É S·ª¨A: Truy·ªÅn YOLO_MODEL v√†o h√†m x·ª≠ l√Ω ·∫£nh üõë
        result_img_rgb, predicted_label = process_static_wheel_image(uploaded_wheel_file, W_WHEEL, b_WHEEL, X_mean_WHEEL, X_std_WHEEL, CLASS_NAMES_WHEEL, YOLO_MODEL)
        st.markdown("---")
        col_img, col_res = st.columns([2, 1])
        with col_img:
            st.image(result_img_rgb, caption="·∫¢nh ƒë√£ x·ª≠ l√Ω (V√¥ lƒÉng, Tay)", use_container_width=True)
        with col_res:
            st.success("‚úÖ D·ª± ƒëo√°n Ho√†n t·∫•t")
            st.metric(label="V·ªã tr√≠ Tay D·ª± ƒëo√°n", value=predicted_label.upper())
            st.caption("Ki·ªÉm tra m√†u s·∫Øc: Xanh l√° (On-wheel), ƒê·ªè (Off-wheel)")
    else:
        st.info("Vui l√≤ng t·∫£i l√™n m·ªôt ·∫£nh l√™n ƒë·ªÉ ki·ªÉm tra v·ªã tr√≠ tay.")
